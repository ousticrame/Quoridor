{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSP Wordle Solver Explanation\n",
    "\n",
    "### 🔍 Overview\n",
    "Our Wordle solver uses a Constraint Satisfaction Problem (CSP) approach combined with letter frequency heuristics to efficiently guess the target word. Here's how it works:\n",
    "\n",
    "#### 🔧 Technical choices\n",
    "\n",
    "For the CSP Solver, we chose to use the Google OR-TOOLS **CP-SAT Solver**, which is one of the best solvers available.\n",
    "For the language, we chose Python as it is one of the most flexible and offers a lot of different libraries for CSP and data processing.   \n",
    "\n",
    "### ⚙️ Key Components\n",
    "\n",
    "#### 1. Word Statistics Initialization\n",
    "- **Positional Frequency**: Tracks how often each letter appears in each position\n",
    "- **Global Letter Frequency**: Tracks overall letter usage across all words\n",
    "\n",
    "#### 2. Word Filtering Function\n",
    "The `filter_valid_words()` function eliminates invalid words based on Wordle feedback:\n",
    "- **Green (G)**: Letter must be in exact position\n",
    "- **Yellow (Y)**: Letter must exist but in different position\n",
    "- **Black/Gray (B)**: Letter doesn't exist (unless duplicated in green/yellow positions)\n",
    "\n",
    "#### 3. Letter constraints and feedback\n",
    "The `get_feedback()` function is first used to first get the feedback of each letters (Green, Yellow and Gray).\\\n",
    "Then, the `update_model()` function is used to update the letter constraints of the model based on the given feedback. This function also updates the heuristic function with the new set of possible words.\n",
    "\n",
    "#### 3. Main Solving Loop\n",
    "1. Start with complete word list\n",
    "2. For each attempt (max 6):\n",
    "   - Filter valid words + add letter constraints + add heuristic function\n",
    "   - Solve model and make guess\n",
    "   - Get feedback by comparing with target\n",
    "   - If all green (solved), exit\n",
    "   - Else continue and update the model\n",
    "\n",
    "### 🎲 Heuristic Strategy\n",
    "The solver uses a weighted scoring system that balances:\n",
    "- Common letter positions\n",
    "- Common letters overall\n",
    "- Letter diversity\n",
    "\n",
    "This approach typically solves Wordle puzzles in 3-4 attempts by:\n",
    "1. Starting with statistically optimal words\n",
    "2. Quickly narrowing down possibilities using feedback\n",
    "3. Avoiding duplicate letters in early guesses\n",
    "\n",
    "### 📝 Example of resolution\n",
    "\n",
    "```\n",
    "Random word: seats\n",
    "\n",
    "Length of possible words: 15921\n",
    "status = OPTIMAL\n",
    "\n",
    "Attempt 1: tares → ['Y', 'Y', 'B', 'Y', 'G']\n",
    "Length of possible words: 16\n",
    "status = OPTIMAL\n",
    "\n",
    "Attempt 2: neats → ['B', 'G', 'G', 'G', 'G']\n",
    "Length of possible words: 7\n",
    "status = OPTIMAL\n",
    "\n",
    "Attempt 3: seats → ['G', 'G', 'G', 'G', 'G']\n",
    "Solved seats in 3 attempts!\n",
    "```\n",
    "\n",
    "> **NOTE**\\\n",
    "> We can see that the model choose `tares` as the first word. An appropriate first guess, containing frequent letters with a coherent placement.\\\n",
    "> It thens takes in account the constraints and makes other updated guesses.\n",
    "\n",
    "### 🚀 Performance\n",
    "The solver can usually find the solution within 3-5 attempts, making it comparable to human performance.\n",
    "The constraints and heuristic makes it guess appropriate words given the context.\n",
    "Guesses the word between 5 to 10 seconds on average.\n",
    "However, given specific heuristic parameters and edge case words, the model can fail to guess the word in 5 attempts.\n",
    "\n",
    "### 📈 Possible improvements\n",
    "- Optimize coefficients for heuristic function, maybe using neural networks or other systems.\n",
    "- Use a bigger dataset\n",
    "- Optimize the code efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hybrid Solver (CSP + LLM)\n",
    "\n",
    "Our Hybrid Solver, is a solver that combines Constraint Satisfaction Problem (CSP) techniques with a Language Learning Model (LLM) to efficiently solve Wordle puzzles. Here's how it works:\n",
    "\n",
    "1. Language Agent (LLM Component)\n",
    "    - Uses OpenAI's model (GPT 4o) to make strategic word selections\n",
    "    - Analyzes patterns and letter distributions in remaining candidates\n",
    "    - Calculates information gain for potential guesses\n",
    "    - Provides explanations for word choices\n",
    "\n",
    "2. CSP Component:\n",
    "    - The Constraint Satisfaction Problem component handles the logical filtering of the word list.\n",
    "    - Maintains constraints using the WordleConstraints class (green, yellow, grey letters)\n",
    "    - Tracks minimum letter counts to handle duplicate letters correctly,\n",
    "    - Efficiently filters the word list after each guess via the filter_valid_words function,\n",
    "    - Ensures all candidates are valid according to game rules,\n",
    "\n",
    "3. Solving Process in Detail:,\n",
    "    - **Initialization**: Starts with a complete dictionary of valid words,\n",
    "    - **First Move Optimization**: For large dictionaries, uses \\\"crane\\\" as a statistically efficient first guess,\n",
    "    - **Iterative Guessing**:,\n",
    "        * The LLM agent suggests optimal next words through function-calling capabilities,\n",
    "        * Feedback is collected comparing the guess against the target word\n",
    "        * CSP filters the dictionary to maintain only valid candidates\n",
    "        * Process repeats until solution is found or max attempts reached\n",
    "\n",
    "4. Example of Solving Flow:\n",
    "    1. Start with complete dictionary (e.g., 2,315 words),\n",
    "    2. First guess \\\"crane\\\" → get feedback (e.g., 🟨⬛🟩⬛⬛),\n",
    "    3. CSP filters dictionary to words matching constraints (e.g., 42 words)\n",
    "    4. LLM analyzes remaining candidates, suggests optimal word through function calling\n",
    "    5. Process repeats until solution is found or we reach 6 guesses\n",
    "\n",
    "## 🔍 How does the LLM make Strategic Decisions\n",
    "1. Strategic Decision Making:\n",
    "    - **Small Candidate Lists** (≤3 words):\n",
    "        * Performs direct information gain calculation for each candidate\n",
    "        * Selects the word that would provide maximum information\n",
    "        - **Larger Candidate Lists**:\n",
    "          * LLM performs deeper analysis using multiple function calls:\n",
    "            - evaluate_information_gain: Calculates expected entropy reduction\n",
    "            - analyze_letter_patterns: Examines letter frequencies and positional patterns\n",
    "            - explain_choice: Provides reasoning behind the selection, which can be seen in the frontend\n",
    "\n",
    "2. Information Gain Calculation:\n",
    "    - Computes initial entropy of remaining word list\n",
    "    - For each potential guess, simulates all possible feedback patterns\n",
    "    - Groups remaining candidates by feedback pattern\n",
    "    - Calculates expected entropy after guess\n",
    "    - Information gain = initial entropy - expected entropy\n",
    "\n",
    "📝 Example:\n",
    "- Imagine we have 8 possible words remaining: [\"crate\", \"grade\",\"crane\", \"trace\", \"space\", \"brace\", \"grace\", \"track\"]\n",
    "\n",
    "If the LLM wants to calculate the information gain for guessing \"crane\":\n",
    "- Calculate initial entropy:\n",
    "    * 8 possible words = log₂(8) = 3 bits of uncertainty\n",
    "- Simulate feedback for \"crane\" against each possible target:\n",
    "    * For \"crate\": 🟩🟩⬛🟨⬛ (feedback pattern A)\n",
    "    * For \"grade\": ⬛🟩⬛🟨⬛ (feedback pattern B)\n",
    "    * For \"crane\": 🟩🟩🟩🟩🟩 (feedback pattern C)\n",
    "    * For \"trace\": 🟨🟩⬛🟨⬛ (feedback pattern D)\n",
    "    * For \"space\": ⬛⬛🟩⬛⬛ (feedback pattern E)\n",
    "    * For \"brace\": ⬛🟩🟩⬛⬛ (feedback pattern F)\n",
    "    * For \"grace\": ⬛🟩🟩⬛⬛ (feedback pattern F again)\n",
    "    * For \"track\": ⬛⬛🟩⬛🟨 (feedback pattern G)\n",
    "- Group words by feedback pattern:\n",
    "    * Pattern A: [\"crate\"] (1 word)\n",
    "    * Pattern B: [\"grade\"] (1 word)\n",
    "    * Pattern C: [\"crane\"] (1 word)\n",
    "    * Pattern D: [\"trace\"] (1 word)\n",
    "    * Pattern E: [\"space\"] (1 word)\n",
    "    * Pattern F: [\"brace\", \"grace\"] (2 words)\n",
    "    * Pattern G: [\"track\"] (1 word)\n",
    "\n",
    "- Calculate expected entropy after guess:\n",
    "    * P(A) = 1/8, entropy = log₂(1) = 0\n",
    "    * P(B) = 1/8, entropy = log₂(1) = 0\n",
    "    * P(C) = 1/8, entropy = log₂(1) = 0\n",
    "    * P(D) = 1/8, entropy = log₂(1) = 0\n",
    "    * P(E) = 1/8, entropy = log₂(1) = 0\n",
    "    * P(F) = 2/8, entropy = log₂(2) = 1\n",
    "    * P(G) = 1/8, entropy = log₂(1) = 0\n",
    "\n",
    "Expected entropy = (1/8 × 0) + (1/8 × 0) + (1/8 × 0) + (1/8 × 0) + (1/8 × 0) + (2/8 × 1) + (1/8 × 0) = 0.25\n",
    "\n",
    "Information gain = Initial entropy - Expected entropy = 3 - 0.25 = 2.75 bits\n",
    "\n",
    "- This means \"crane\" gives us 2.75 bits of information, which is very good (close to the maximum possible 3 bits). After guessing \"crane\", we'll likely narrow down to just 1 or 2 possible words.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
